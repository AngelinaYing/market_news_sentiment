{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ange/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ange/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tqdm.pandas()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"news_data/news_data_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Convert string representations of lists back into actual lists\n",
    "news['headline_lemmas'] = news['headline_lemmas'].apply(ast.literal_eval)\n",
    "news['snippet_lemmas'] = news['snippet_lemmas'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary of words from the corpus\n",
    "dictionary = gensim.corpora.Dictionary(news['headline_lemmas'])\n",
    "\n",
    "# Filter the dictionary to meet frequency thresholds\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=10000)\n",
    "\n",
    "# Convert the document into the bag-of-words format\n",
    "corpus_headline = [dictionary.doc2bow(doc) for doc in news['headline_lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeadlineTopic 0: nyt(0.0170) front(0.0041) page(0.0041) men(0.0016) weekend(0.0015) facebook(0.0014) clinton(0.0013) campaign(0.0013) climate(0.0013) de(0.0013)\n",
      "HeadlineTopic 1: climate(0.0018) weekend(0.0017) debate(0.0017) iran(0.0015) clinton(0.0015) market(0.0014) impeachment(0.0013) men(0.0013) turn(0.0013) protest(0.0013)\n",
      "HeadlineTopic 2: clinton(0.0019) fashion(0.0019) weekend(0.0016) climate(0.0016) debate(0.0014) spring(0.0014) street(0.0014) men(0.0014) u(0.0014) market(0.0014)\n",
      "HeadlineTopic 3: clinton(0.0030) hillary(0.0023) weekend(0.0022) obama(0.0019) spring(0.0017) shooting(0.0016) debate(0.0015) brooklyn(0.0014) fashion(0.0014) sander(0.0014)\n",
      "HeadlineTopic 4: spring(0.0019) weekend(0.0017) college(0.0016) iran(0.0015) monday(0.0014) climate(0.0014) u(0.0014) market(0.0013) made(0.0013) russian(0.0013)\n",
      "HeadlineTopic 5: weekend(0.0024) debate(0.0016) syria(0.0016) market(0.0016) clinton(0.0015) obama(0.0015) michael(0.0014) brooklyn(0.0014) supreme(0.0014) last(0.0014)\n",
      "HeadlineTopic 6: weekend(0.0024) thursday(0.0017) brexit(0.0016) men(0.0015) market(0.0014) film(0.0013) clinton(0.0013) artist(0.0013) facebook(0.0012) spring(0.0012)\n",
      "HeadlineTopic 7: debate(0.0019) fashion(0.0019) clinton(0.0017) shooting(0.0015) market(0.0014) another(0.0014) u(0.0014) night(0.0014) weekend(0.0014) care(0.0013)\n",
      "HeadlineTopic 8: clinton(0.0019) brexit(0.0015) college(0.0015) job(0.0015) fashion(0.0015) test(0.0014) brooklyn(0.0014) nyt(0.0014) gun(0.0014) tuesday(0.0014)\n",
      "HeadlineTopic 9: clinton(0.0024) friday(0.0018) fashion(0.0017) obama(0.0016) men(0.0016) hillary(0.0015) college(0.0015) brooklyn(0.0015) weekend(0.0015) vote(0.0014)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "lda_headline = gensim.models.LdaMulticore(corpus_headline, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "topics_headline = lda_headline.show_topics(num_topics=10, num_words=10, log=True, formatted=False)\n",
    "\n",
    "# print the top words associated with each topic (top 10)\n",
    "for topic_num, topic_words in topics_headline:\n",
    "  topic_rep = \" \".join ([f\"{w}({p:.4f})\" for w,p in topic_words])\n",
    "  print (f\"HeadlineTopic {topic_num}: {topic_rep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnippetTopic 0: nyt(0.0027) front(0.0017) page(0.0017) guide(0.0015) bride(0.0013) groom(0.0013) tuesday(0.0013) saturday(0.0013) clinton(0.0013) thursday(0.0012)\n",
      "SnippetTopic 1: tuesday(0.0019) wednesday(0.0016) friday(0.0016) thursday(0.0014) guide(0.0014) obama(0.0012) fashion(0.0012) monday(0.0012) clinton(0.0011) saturday(0.0011)\n",
      "SnippetTopic 2: friday(0.0015) guide(0.0014) wednesday(0.0014) monday(0.0012) fashion(0.0012) tuesday(0.0012) clinton(0.0012) thursday(0.0011) saturday(0.0011) designer(0.0010)\n",
      "SnippetTopic 3: tuesday(0.0018) monday(0.0018) friday(0.0012) fashion(0.0011) thursday(0.0011) spring(0.0011) bride(0.0011) wednesday(0.0011) shooting(0.0010) writes(0.0010)\n",
      "SnippetTopic 4: tuesday(0.0020) monday(0.0016) wednesday(0.0014) clinton(0.0014) guide(0.0014) ahead(0.0013) thursday(0.0013) shooting(0.0011) saturday(0.0010) robert(0.0009)\n",
      "SnippetTopic 5: monday(0.0015) clinton(0.0015) wednesday(0.0014) guide(0.0014) thursday(0.0014) friday(0.0012) saturday(0.0011) spring(0.0011) tuesday(0.0011) obama(0.0010)\n",
      "SnippetTopic 6: monday(0.0017) friday(0.0016) tuesday(0.0014) groom(0.0013) bride(0.0012) clinton(0.0011) thursday(0.0011) saturday(0.0011) james(0.0010) supreme(0.0010)\n",
      "SnippetTopic 7: friday(0.0018) thursday(0.0017) clinton(0.0015) tuesday(0.0013) guide(0.0013) property(0.0012) wednesday(0.0012) saturday(0.0012) fashion(0.0011) ahead(0.0011)\n",
      "SnippetTopic 8: friday(0.0017) tuesday(0.0016) clinton(0.0016) thursday(0.0014) guide(0.0013) fashion(0.0012) ahead(0.0011) wednesday(0.0010) designer(0.0010) hillary(0.0010)\n",
      "SnippetTopic 9: tuesday(0.0018) wednesday(0.0017) thursday(0.0016) monday(0.0015) clinton(0.0013) guide(0.0012) friday(0.0012) obama(0.0012) designer(0.0010) photographer(0.0010)\n"
     ]
    }
   ],
   "source": [
    "# Construct a dictionary of words from the 'snippet_lemmas'\n",
    "dictionary_snippet = gensim.corpora.Dictionary(news['snippet_lemmas'])\n",
    "\n",
    "# Filter the dictionary to meet frequency thresholds\n",
    "dictionary_snippet.filter_extremes(no_below=10, no_above=0.5, keep_n=10000)\n",
    "\n",
    "# Convert the document into the bag-of-words format\n",
    "corpus_snippet = [dictionary_snippet.doc2bow(doc) for doc in news['snippet_lemmas']]\n",
    "\n",
    "# Fit LDA model on the snippet corpus\n",
    "lda_snippet = gensim.models.LdaMulticore(corpus_snippet, num_topics=10, id2word=dictionary_snippet, passes=2, workers=2)\n",
    "topics_snippet = lda_snippet.show_topics(num_topics=10, num_words=10, log=True, formatted=False)\n",
    "\n",
    "# Print the top words associated with each topic for snippets\n",
    "for topic_num, topic_words in topics_snippet:\n",
    "  topic_rep = \" \".join([f\"{w}({p:.4f})\" for w,p in topic_words])\n",
    "  print(f\"SnippetTopic {topic_num}: {topic_rep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the topic distributions\n",
    "headlines_topics = []\n",
    "\n",
    "# Iterate over each document in the corpus\n",
    "for doc_bow in corpus_headline:\n",
    "    # Get the topic distribution for the document\n",
    "    doc_topics = lda_headline.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "    # Extract the topic probabilities\n",
    "    topic_probs = [prob for _, prob in doc_topics]\n",
    "    # Append to the list\n",
    "    headlines_topics.append(topic_probs)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "headlines_topic_df = pd.DataFrame(headlines_topics, columns=[f\"Headline_Topic_{i}\" for i in range(10)])\n",
    "\n",
    "# Add the publication date for reference\n",
    "headlines_topic_df['pub_date'] = news['pub_date'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the topic distributions for snippets\n",
    "snippets_topics = []\n",
    "\n",
    "# Iterate over each document in the snippet corpus\n",
    "for doc_bow in corpus_snippet:\n",
    "    # Get the topic distribution for the document\n",
    "    doc_topics = lda_snippet.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "    # Extract the topic probabilities\n",
    "    topic_probs = [prob for _, prob in doc_topics]\n",
    "    # Append to the list\n",
    "    snippets_topics.append(topic_probs)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "snippets_topic_df = pd.DataFrame(snippets_topics, columns=[f\"Snippet_Topic_{i}\" for i in range(10)])\n",
    "\n",
    "# Add the publication date for reference\n",
    "snippets_topic_df['pub_date'] = news['pub_date'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the topic distributions for snippets\n",
    "snippets_topics = []\n",
    "\n",
    "# Iterate over each document in the snippet corpus\n",
    "for doc_bow in corpus_snippet:\n",
    "    # Get the topic distribution for the document\n",
    "    doc_topics = lda_snippet.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "    # Extract the topic probabilities\n",
    "    topic_probs = [prob for _, prob in doc_topics]\n",
    "    # Append to the list\n",
    "    snippets_topics.append(topic_probs)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "snippets_topic_df = pd.DataFrame(snippets_topics, columns=[f\"Snippet_Topic_{i}\" for i in range(10)])\n",
    "\n",
    "# Add the publication date for reference\n",
    "snippets_topic_df['pub_date'] = news['pub_date'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the headline topics with the original DataFrame\n",
    "news_with_headline_topics = pd.merge(news, headlines_topic_df, on='pub_date', how='left')\n",
    "\n",
    "# Merge the snippet topics with the DataFrame that already has headline topics\n",
    "final_news_df = pd.merge(news_with_headline_topics, snippets_topic_df, on='pub_date', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_news = final_news_df.to_csv(\"news_data/lda.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
